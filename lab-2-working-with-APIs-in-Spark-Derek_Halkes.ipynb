{"cells":[{"cell_type":"code","source":["#importing libraries\nimport json\nfrom os import getenv\nfrom pyspark.sql import Row, SparkSession\nfrom pyspark.sql.functions import expr\n\n#not used in batch processing\n#from alpaca_trade_api.stream import Stream\n\nfrom alpaca_trade_api.rest import REST, TimeFrame, TimeFrameUnit\nfrom delta.tables import DeltaTable"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"cd3811a9-84c8-4c60-85b6-92869edf4947"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["#creating Spark session\nspark = SparkSession.builder.appName('DSSA5102').getOrCreate()\nspark.getActiveSession()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"00fad66f-4879-4ce6-a25b-12b2697e4095"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"\n            <div>\n                <p><b>SparkSession - hive</b></p>\n                \n        <div>\n            <p><b>SparkContext</b></p>\n\n            <p><a href=\"/?o=3473015989495363#setting/sparkui/1125-150200-qf5aj2xd/driver-7596051310931688814\">Spark UI</a></p>\n\n            <dl>\n              <dt>Version</dt>\n                <dd><code>v3.2.0</code></dd>\n              <dt>Master</dt>\n                <dd><code>local[8]</code></dd>\n              <dt>AppName</dt>\n                <dd><code>Databricks Shell</code></dd>\n            </dl>\n        </div>\n        \n            </div>\n        ","textData":null,"removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"htmlSandbox","arguments":{}}},"output_type":"display_data","data":{"text/html":["\n            <div>\n                <p><b>SparkSession - hive</b></p>\n                \n        <div>\n            <p><b>SparkContext</b></p>\n\n            <p><a href=\"/?o=3473015989495363#setting/sparkui/1125-150200-qf5aj2xd/driver-7596051310931688814\">Spark UI</a></p>\n\n            <dl>\n              <dt>Version</dt>\n                <dd><code>v3.2.0</code></dd>\n              <dt>Master</dt>\n                <dd><code>local[8]</code></dd>\n              <dt>AppName</dt>\n                <dd><code>Databricks Shell</code></dd>\n            </dl>\n        </div>\n        \n            </div>\n        "]},"transient":null}],"execution_count":0},{"cell_type":"code","source":["#activating environmentals variables that are attached to the cluster\nalpaca_api_key = getenv(\"APCA_API_KEY_ID\")\nalpaca_secret_key = getenv(\"APCA_API_SECRET_KEY\")\nalpaca_base_url = getenv(\"APCA_API_BASE_URL\")\n\n#similar to an HTTP call\napi = REST(alpaca_api_key, alpaca_secret_key, api_version='v2')"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4ce1d61c-6ce1-47d7-8237-0e95364a25a1"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["#getting count of all stock tickers (this is the bronze portion of the objective)\n\n# Create an empty list for results\ndata = []\n\n# Use the API to get a list of all available stock tickers\nassets = api.list_assets()\n\n# Traverse the list of tickers, applying a filter for only those that are tradable\n# Format the results into a Spark Row(), then append the result to the list called data\nfor ticker in assets:\n  if ticker.tradable == True:\n     data.append(\n       Row(\n         id= ticker.id,\n         easy_to_borrow= ticker.easy_to_borrow,\n         exchange= ticker.exchange,\n         fractionable= ticker.fractionable, \n         marginable= ticker.marginable,\n         name= ticker.name,\n         shortable= ticker.shortable,\n         status= ticker.status,\n         symbol= ticker.symbol,\n         tradable = ticker.tradable)\n     )\n\n# Create a Spark Dataframe using the list called data\ntradable_df = spark.createDataFrame(data)\n\n# Display a sample of the results and print total rows\ndisplay(tradable_df)\nprint(\"Total Number of Tradable Ticker Symbols\",tradable_df.count())"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"843e845a-a9f2-437e-849d-370264dc2951"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["#delta does ACID guarantees, tracks changes\n\n# DBTITLE 1,Write Tradable Stock Assets to a Delta Table & Add the Delta Table to the Hive Metastore\n# Get or create target delta table\nif (DeltaTable.isDeltaTable(spark, \"/alpaca/assets/\")):\n    # Load delta table\n    deltaTable = DeltaTable.forPath(spark, \"/alpaca/assets/\")\n    # Merge new or changed data into target\n    deltaTable.alias('delta') \\\n        .merge(\n        source=tradable_df.alias('updates'),\n        condition=expr(f\"delta.id = updates.id\")\n    ) \\\n        .whenMatchedUpdateAll() \\\n        .whenNotMatchedInsertAll() \\\n        .execute()\n    # Clean up old files\n    deltaTable.vacuum()\n\nelse:\n    tradable_df \\\n        .write \\\n        .format('delta') \\\n        .save(\"/alpaca/assets/\")\n\n# Create table in hive metastore\nif not spark._jsparkSession.catalog().tableExists(\"alpaca.assets\"):\n    spark.sql(f\"CREATE DATABASE IF NOT EXISTS alpaca\")\n    spark.sql(f\"CREATE TABLE IF NOT EXISTS alpaca.assets USING DELTA LOCATION '/alpaca/assets/'\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"62bbdae3-bee0-4935-a89d-45de3720ad18"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# DBTITLE 1,Get the Historical Hourly Price Data for the last Year for each Tradable Stock\n# The Alpaca API serve polling request as paginated results with a limit of 10000 rows\n# A single year contains 8760 hours keeping us below request limits per ticker\n# The Free Alpaca APIs do not support more 200 calls per minute, so executing without concurrency will take some time to run\nfrom pyspark.sql.types import StructType,StructField, StringType, TimestampType, DecimalType, IntegerType\n\ndef process_bars(bar, symbol):\n  values = bar.__dict__.pop('_raw')\n  bar = dict()\n  bar['symbol'] = symbol\n  bar['timestamp'] = values.pop('t')\n  bar['open'] = values.pop('o')\n  bar['high'] = values.pop('h')\n  bar['low'] = values.pop('l')\n  bar['close'] = values.pop('c')\n  bar['volume'] = values.pop('v')\n  bar['trade_count'] = values.pop('n')\n  bar['vwap'] = values.pop('vw')\n  json.dumps(bar)\n  return bar\n\nbar_list = []\n\n# If we do this for every stock we pulled from the apis, the dataset will grow to ~14mil rows\n# so we will work with a sample for our lab. This will prevent us from having to make concurrent API calls\n# and consuming more resources than what is available using single node spark\nseed = 10\nsample = tradable_df.sample(False, .01, seed)\n\nfor row in range(sample.count()):\n  symbol = sample.collect()[row][-2]\n  bar_iter = api.get_bars_iter(symbol, TimeFrame.Hour, start=\"2020-01-01\", end=\"2021-01-01\")\n  for bar in bar_iter:\n    bar = process_bars(bar, symbol)\n    bar_list.append(bar)\n  \nprices_df = spark.read.json(sc.parallelize(bar_list))\nprices_df.count() \ndisplay(prices_df) "],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6287da5d-401b-48f0-81be-cb1df1e4ff2e"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# DBTITLE 1,Write Sampled Stock Assets Hourly Market Data to a Delta Table & Add the Delta Table to the Hive Metastore\n# Get or create target delta table\nif (DeltaTable.isDeltaTable(spark, \"/alpaca/bars/\")):\n    # Load delta table\n    deltaTable = DeltaTable.forPath(spark, \"/alpaca/bars/\")\n    # Merge new or changed data into target\n    deltaTable.alias('delta') \\\n        .merge(\n        source=prices_df.alias('updates'),\n        condition=expr(f\"delta.symbol = updates.symbol and delta.timestamp=updates.timestamp\")\n    ) \\\n        .whenMatchedUpdateAll() \\\n        .whenNotMatchedInsertAll() \\\n        .execute()\n    # Clean up old files\n    deltaTable.vacuum()\n\nelse:\n      prices_df \\\n        .write \\\n        .partitionBy(\"symbol\") \\\n        .format('delta') \\\n        .save(\"/alpaca/bars/\")\n\n# Create table in hive metastore\nif not spark._jsparkSession.catalog().tableExists(\"alpaca.bars\"):\n    spark.sql(f\"CREATE DATABASE IF NOT EXISTS alpaca\")\n    spark.sql(f\"CREATE TABLE IF NOT EXISTS alpaca.bars USING DELTA LOCATION '/alpaca/bars/'\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c4449367-54d9-40ee-a774-f248338c3e0d"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["#importing libraries for batch processing\nfrom pyspark.sql import SparkSession, Window\nfrom pyspark.sql import functions as f\nfrom delta.tables import DeltaTable"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"81e98f29-ecaf-457a-a973-ce482da437cf"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# DBTITLE 1,Get or Create a New Spark Session\nspark = SparkSession.builder.appName('alpaca-batch').getOrCreate()\nspark.getActiveSession()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"862b1a9f-1560-4fe4-8714-d1ce186be59c"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"\n            <div>\n                <p><b>SparkSession - hive</b></p>\n                \n        <div>\n            <p><b>SparkContext</b></p>\n\n            <p><a href=\"/?o=3473015989495363#setting/sparkui/1125-150200-qf5aj2xd/driver-7596051310931688814\">Spark UI</a></p>\n\n            <dl>\n              <dt>Version</dt>\n                <dd><code>v3.2.0</code></dd>\n              <dt>Master</dt>\n                <dd><code>local[8]</code></dd>\n              <dt>AppName</dt>\n                <dd><code>Databricks Shell</code></dd>\n            </dl>\n        </div>\n        \n            </div>\n        ","textData":null,"removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"htmlSandbox","arguments":{}}},"output_type":"display_data","data":{"text/html":["\n            <div>\n                <p><b>SparkSession - hive</b></p>\n                \n        <div>\n            <p><b>SparkContext</b></p>\n\n            <p><a href=\"/?o=3473015989495363#setting/sparkui/1125-150200-qf5aj2xd/driver-7596051310931688814\">Spark UI</a></p>\n\n            <dl>\n              <dt>Version</dt>\n                <dd><code>v3.2.0</code></dd>\n              <dt>Master</dt>\n                <dd><code>local[8]</code></dd>\n              <dt>AppName</dt>\n                <dd><code>Databricks Shell</code></dd>\n            </dl>\n        </div>\n        \n            </div>\n        "]},"transient":null}],"execution_count":0},{"cell_type":"code","source":["# DBTITLE 1,Read the Alpaca Bar (Silver) Delta Table \nbars = spark.table('alpaca.bars')\ndisplay(bars)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8d23147c-754d-4d21-adc1-9eed995fc2e1"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["bars = bars \\\n  .withColumn(\"year\", f.year(f.col('timestamp'))) \\\n  .withColumn(\"month\", f.month(f.col('timestamp'))) \\\n  .withColumn(\"day\", f.dayofmonth(f.col('timestamp'))) \\\n  .withColumn(\"dayofweek\", f.dayofweek(f.col('timestamp'))) \\\n  .withColumn('dayofyear', f.dayofyear(f.col('timestamp'))) \\\n  .withColumn('hour', f.hour('timestamp')) \\\n\ndisplay(bars)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d57d5872-dc52-42e7-b001-40c23676376d"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# DBTITLE 1,Lag Based Features\nspec = Window.partitionBy(\"symbol\").orderBy('timestamp')\n\nlags = [i for i in range(1,6)]\ncolumns = ['close', 'high', 'low', 'open', 'trade_count', 'volume']\n\nfor col in columns:\n  for l in lags:\n    bars = bars \\\n        .withColumn(f'{col}_tminus_{l}', f.lag(f.col(col), offset=l).over(spec))\n\ndisplay(bars)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d807bd08-53ad-4fb2-925e-6043bb0d2c90"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# DBTITLE 1,Analytic Function Based Features\nbars = bars.withColumn(\"cume_dist\",f.cume_dist().over(spec))\n\ndisplay(bars)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"06331153-13cd-4b6f-936a-cb7b5e7f4c1f"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# DBTITLE 1,Rolling and Expanding Window Based Features\nspec = Window.partitionBy(\"symbol\").orderBy('timestamp')\nrolling = Window.partitionBy(\"symbol\").orderBy('timestamp').rowsBetween(-3, 0)\n\ncolumns = ['close', 'high', 'low', 'open', 'trade_count', 'volume']\n\nfor col in columns:\n  bars = bars \\\n    .withColumn(f\"{col}_avg\", f.avg(f.col(col)).over(spec)) \\\n    .withColumn(f\"{col}_min\", f.min(f.col(col)).over(spec)) \\\n    .withColumn(f\"{col}_max\", f.max(f.col(col)).over(spec)) \\\n    .withColumn(f\"{col}_3h_rolling_avg\", f.avg(f.col(col)).over(rolling)) \\\n    .withColumn(f\"{col}_3h_rolling_min\", f.min(f.col(col)).over(rolling)) \\\n    .withColumn(f\"{col}_3h_rolling_max\", f.max(f.col(col)).over(rolling)) \n\ndisplay(bars)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e55a8bbf-64f1-4a87-84d4-f7a1f8da14e0"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# DBTITLE 1,Target Feature\nbars = bars.withColumn('target',f.lead(f.col('close'), offset=6).over(spec))\n\ndisplay(bars)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"12b65840-f13b-47f5-8bb7-66204ef3ca3d"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# DBTITLE 1,Write Feature Table (Gold) to Delta Lake\n# Get or create target delta table\nif (DeltaTable.isDeltaTable(spark, \"/feature_store/stock_prices\")):\n    # Load delta table\n    deltaTable = DeltaTable.forPath(spark, \"/feature_store/stock_prices\")\n    # Merge new or changed data into target\n    deltaTable.alias('delta') \\\n        .merge(\n        source=bars.alias('updates'),\n        condition=expr(f\"delta.symbol = updates.symbol and delta.timestamp=updates.timestamp\")\n    ) \\\n        .whenMatchedUpdateAll() \\\n        .whenNotMatchedInsertAll() \\\n        .execute()\n    # Clean up old files\n    deltaTable.vacuum()\n\nelse:\n      bars \\\n        .write \\\n        .partitionBy(\"symbol\") \\\n        .format('delta') \\\n        .save(\"/feature_store/stock_prices\")\n\n# Create table in hive metastore\nif not spark._jsparkSession.catalog().tableExists(\"feature_store.stock_prices\"):\n    spark.sql(f\"CREATE DATABASE IF NOT EXISTS feature_store\")\n    spark.sql(f\"CREATE TABLE IF NOT EXISTS feature_store.stock_prices USING DELTA LOCATION '/feature_store/stock_prices'\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"87c8afe2-4f6a-4653-8665-fd5376e39ddf"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":[""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d98b8b5f-02a9-48a8-913e-e0444b66fce1"}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"lab-2-working-with-APIs-in-Spark-Derek_Halkes","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{},"notebookOrigID":3148616861449391}},"nbformat":4,"nbformat_minor":0}
